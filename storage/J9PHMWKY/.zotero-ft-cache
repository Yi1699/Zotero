EXTREME VIDEO COMPRESSION WITH PREDICTION USING PRE-TRAINED DIFFUSION MODELS
Bohan Li⋆, Yiming Liu⋆, Xueyan Niu‡, Bo Bai‡, Lei Deng‡, and Deniz G ̈und ̈uz†‡
⋆Xidian University, Shaanxi, China, {bohanli, yimingliu}@stu.xidian.edu.cn †Imperial College London, London, U.K, d.gunduz@imperial.ac.uk ‡Huawei Technologies Co. Ltd., {niuxueyan3, baibo8, deng.lei2}@huawei.com
ABSTRACT
Diffusion models have achieved remarkable success in generating high quality image and video data. More recently, they have also been used for image compression with high perceptual quality. In this paper, we present a novel approach to extreme video compression leveraging the predictive power of diffusion-based generative models at the decoder. The conditional diffusion model takes several neural compressed frames and generates subsequent frames. When the reconstruction quality drops below the desired level, new frames are encoded to restart prediction. The entire video is sequentially encoded to achieve a visually pleasing reconstruction, considering perceptual quality metrics such as the learned perceptual image patch similarity (LPIPS) and the Fre ́chet video distance (FVD), at bit rates as low as 0.02 bits per pixel (bpp). Experimental results demonstrate the effectiveness of the proposed scheme compared to standard codecs such as H.264 and H.265 in the low bpp regime. The results showcase the potential of exploiting the temporal relations in video data using generative models. Code is available at: https://github.com/ElesionKyrie/Extreme-VideoCompression-With-Prediction-Using-Pre-trainded-DiffusionModels
Index Terms— Video Compression, Video Prediction, Diffusion Models
1. INTRODUCTION
Recent years have witnessed an exponential growth in demand for video data, which contain richer spatial and temporal information than other information sources such as image, audio and text. With the emergence of augmented and virtual reality (AR/VR) technologies and metaverse applications, the need for highly compressed video transmission is expected to become ever more crucial. Over the past decades, many video compression methods have been standardized and commercialized, notably H.264/AVC, H.265/HEVC, and H.266/VVC. These compression methods are often hand-engineered with techniques such as intra-frame and
inter-frame compression and entropy coding. The rapid advances in artificial intelligence in recent decades have brought new opportunities to video compression, and many methods have been shown to perform on par or better than traditional codecs by replacing certain components with learned models [1, 2, 3]. Both the traditional methods and the neural compression techniques rely on optical flow and motion vector estimation to capture motion information, which is then used for frame interpolation, followed by residual compression and entropy coding. In parallel, there have been significant progress in generative AI technologies, and their applications to video generation, prediction and in-filling [4, 5, 6, 7]. In this paper, we propose a video compression scheme relying on the generative power of diffusion models [8, 9]. The core idea of the diffusion model is to simulate the information propagation process among pixels in an image by gradually adding Gaussian noise. The network captures the low-dimensional representations, and the reverse process uses the learned features to generate diverse high-quality reconstructions. We use the diffusion model to decode certain video frames by recursively predicting future frames conditioning on encoded frames within specific time windows. The compression scheme ensures that the generated video frames achieve a pre-defined reconstruction quality. A subset of chosen frames is chosen based on the required quality and encoded using neural image codecs. Then they are passed as conditional frames to the video generation model, which generates several subsequent frames. Since only a subset of original frames needs to be encoded and the remaining frames can be generated by prediction, the scheme can achieve an ultra-low bit rate (below 0.02 bpp) with comparative visual quality relative to other standard video compression methods that require optical flow estimation and motion compensation.
2. RELATED WORK
2.1. Video Compression Codecs
Traditional video compression techniques, such as AVC, HEVC, and VVC, are mostly hand-engineered. In recent
arXiv:2402.08934v1 [eess.IV] 14 Feb 2024


years, learning-based video compression has emerged as a new research direction. The attention mechanism used in motion compensation by the deep contextual video compression (DCVC) scheme in [10] is proved to be more effective than the optical flow reverse warping technique employed by the DVC scheme proposed in [11]. The authors of DCVC have further improved their work and ultimately proposed DCVCDC [12]. We will use DCVC-DC as one of the baselines for comparison with the performance of our model. These works have demonstrated remarkable performance, outperforming traditional compression methods like H.264 and H.265 in terms of PSNR and MS-SSIM metrics.
2.2. Video Prediction and Generation
Video prediction and frame interpolation aim to generate new video frames according to existing ones preserving the spatial and temporal coherence. Traditionally, methods such as optical flow estimation and motion vector estimation have been developed to capture motion information which is then used for frame interpolation. Data-driven approaches using GANs and diffusion models have shown promising performance gains in real-world data synthesis. GANs for video generation and prediction have gone through a remarkable development over the last few years and produce impressive results [13]. Diffusion models outperform GANs in an increasing number of tasks and diverse domains and have also been deployed in video generation [14, 15, 16]. Nevertheless, diffusion models are in general computationally more intensive, and as the prediction step increases, the quality of the synthesis deteriorates. In our work, we address this issue by using an autoregressive approach, drawing inspiration from a diffusion-based model called masked conditional video diffusion (MCVD) [17], which processes relatively short video segments each time and iteratively produces the frame generations. MCVD employs a score-based diffusion model, and has been shown to achieve state-of-the-art (SOTA) results on various datasets including SMMNIST, KTH, BAIR, and Cityscapes, while being computationally efficient. For the denoising network, an enhanced U-Net architecture is utilized, integrating 2D convolutions, multi-head self-attention mechanisms, and adaptive instance normalization. MCVD applies a conditioning procedure based on masking past and/or future frames in a blockwise manner. The model generates videos in fixed-step increments, using the generated video as conditioning frames for subsequent generations. MCVD also supports a variety of video tasks, including future/past prediction, unconditional generation, and interpolation.
2.3. Generative models for video compression
Generative models have been employed for video compression to obtain better perceptual quality at the decoder. Mentzer et al. [18] employ GANs in conjunction with the
Distortion/ Perception Threshold
Generative Models
Original Frames
Generated Frames
Decompressed Frames
Generative Models
Neural Compression
Neural Compression
Fig. 1: Method overview. The first few frames are compressed by the encoder, while the following frames are generated using a pre-trained generative model at the decoder. When the generation quality drops below the desired threshold, new frames are encoded to sustain the overall visual quality.
neural compression pipeline used in [11] for improved perceptual quality. While these works provide certain improvements, they do not exhibit gains in terms of LPIPS or FID compared to H.264/H.265. To the best of our knowledge, there is no prior video compression framework that benefits from the power of diffusion models, despite highly promising results for image compression [19, 20, 21].
3. METHODS
The proposed predictive video compression framework consists of two components functioning in an iterative manner: (i) compression of individual frames using neural image compression; and (ii) frame generation using diffusion-based generative models. Thanks to the power of the pre-trained generative model, only a subset of the frames are compressed using neural image compression, while the remaining frames are generated by the pre-trained generative network at the decoder, and hence, require no communications. See Fig. 1 for a high-level depiction of our method.
3.1. Preprocessing
Let A1:T ∈ RT ×C×H×W be a sequence of video frames, where T denotes the number of frames, C denotes the number of channels (C = 3 for RGB), and H, W denote the height and width of the frames. We assume that At evolves over time following the joint distribution p(A1:T ). In the proposed framework, the initial k frames A1:k and a subset of intermediate frames are encoded using a SOTA image compression method. In our implementation, we use the SOTA neural compression scheme ELIC [22] for compression of individual frames.


3.2. Frame generation with diffusion-based models
In standard video coding, and all the neural-based video compression schemes, frames between intra-coded frames are coded in reference to the intra-frames, typically through motion or optical flow estimation. Instead, in the proposed approach, we rely completely on the generative power of the diffusion-based neural networks at the decoder. Let S ⊂ [T ] be the subset of intra-coded frames compressed individually. Then, the remaining frames, for example Ai ∈ A[T ]\S, will be generated using a pre-trained diffusion model with parameter θ, conditioned on the previous k + 1 frames Ai−k−1:i−1. We assume that a model p(At+k+1|At:t+k) is learned by the generative model during the training process. Specifically, let A0 ∈ Rd be a sample from the data distribution pdata. The forward diffusion process (FDP) with defined variance schedule βt from t = 0 to t = T is obtained by adding Gaussian noise terms
q(At|At−1) = N (At; p1 − βtAt−1, βtI).
The reverse diffusion process (RDP) is also defined as a Markov chain with learned Gaussian transitions starting from AT ∼ N (0, I), which is computed using the transition kernel
pθ(AT : 0|y) = p(AT )
T
Y
t=1
pθ(At−1|At, A0)
with pθ(At−1|A0) = N (At−1; μ(At, A0), βetI), where α ̄t : =
Qt
s=1(1−βs), βet = 1−α ̄t−1
1−α ̄t βt, and μt(At, A0) =
√α ̄ t−1 βt
1−α ̄t A0+
√α ̄ t (1−α ̄ t−1 )
1−α ̄t At.
Given a sequence of past k + 1 frames Ai:i+k, a video prediction model, for example, as proposed in [17], is trained to learn the conditional distribution of p(Ai+k+1|Ai:i+k) with the objective Lpred(θ) =
Et,[y,x],ε∼N (0,I)[∥ε − εθ
√α ̄tx + √1 − α ̄tε | y, t ∥2
2].
3.3. The sequential encoding process
The key mechanism of the proposed video compression framework is a decision process at the encoder that strategically excludes a substantial portion of frames according to the pre-trained generative model without compromising the quality of the reconstructed video frames. The pseudo-code of the procedure is presented in Algorithm 1. During the encoding, a window of k + 1 frames Ai:i+k is processed at once, with i ranging from 1 to T − k. A list S ⊆ [T ] of frame number is maintained to track if the current frame is encoded or if it will be generated at the decoder using the previous frames. We initialize this list with the first k frames, such that [k] ⊆ S. Specifically, at time t > k, an estimation Aet+1:t+j is derived using the given generative model, which takes as input the frames Aet−k:t and predicts the next
j frames
Aet+1:t+j = arg max pθ(At+1:t+j |Aet−k:t)
=: Gθ(Aet−k:t)
according to the learned conditional distribution pθ. The result is then compared to the uncompressed data with a given distortion/perception threshold ρ > 0. Let D(·, ·) be the similarity metric. For each frame of the generated frames, if it meets the thresholding quality requirements, i.e., if D(At+i, Aet+i) < ρ, where 0 ≤ i ≤ j, we remove the frame by adding t + 1 to the set [T ] \ S. Otherwise, if D(At+i, Aet+i) ≥ ρ, where 0 ≤ i ≤ j, we add [t + 1 : t + k] to the set S, which means that the frame At+1:t+k is encoded using the codec. The procedure is conducted sequentially until t = T − 1. During decoding, the frames AS are decoded using the corresponding decoder of the codec used for compressing intra frames. For the remaining frames, each Aˆi, i ∈ [T ] \ S is generated by the pre-trained generative
network taking Aˆi−k−1:i−1 as condition. The thresholding process ensures that the quality of the aggregated decompression meets the given quality requirement ρ concerning the metric D(·, ·).
Algorithm 1 Sequential encoding with codec and pre-trained neural network Input: Original video frames A1:T , Quality threshold ρ, Neural Image encoding method Enc(·), Neural Image decoding method Dec(·), Pre-trained generative model Gθ(·) Parameters: Length of video T , Lengths of generation windows k and j Output: List of frame index S , List of frames A encoded using neural codec
1: S ← [k], A ← Enc(A1:k), l ← k, m ← k 2: while l ≤ T − 1 do
3: Ael+1:l+j = Gθ(Ael−k:l)
4: for i = 1 to j do 5: if D(Ael+i, Al+i) < ρ then 6: A ← A ∪ {∗} 7: m ← m + 1 8: else
9: S ← S ∪ [l + i : l + i + k]
10: A ← A ∪ {Enc(Al+i:l+i+k)} 11: m ← m + k 12: Break; 13: end if 14: end for 15: l ← m 16: end while 17: return S, A


4. EXPERIMENTAL RESULTS
4.1. Experimental Setup
Dataset: We conducted experiments on three datasets: Stochastic Moving MNIST (SMMNIST) dataset [23], Cityscapes dataset [24], and Ultra Video Group (UVG) dataset [25]. The first two datasets are widely used in the domain of video prediction, while the UVG dataset has found extensive applications in the field of video compression. The SMMNIST dataset is an extension of the MNIST dataset, where the digit images form a temporal sequence, creating dynamic scenes as a result of random movements of the black-and-white digits, and we use the original resolution of 64 × 64. The cityscapes dataset contains high-resolution images of different city streets recorded under various conditions, simulating the diversity of real world. The dataset has already been split into training and testing, so we randomly sample 720 frames from the test set. We center-crop and down-sample the original frames to a resolution of 128×128. In our experiments, each video consists of 30 frames. The UVG dataset comprises 16 video sequences of 3840×2160. For our experiments, we selected 7 videos with a frame rate of 120 frames per second (fps). Similar to the Cityscapes dataset, we resized these video frames to 128x128 and took the initial 30 frames. Evaluation Metrics: Conventional metrics for video compression, such as the peak signal-to-noise ratio (PSNR) and the structural similarity index measure (SSIM), evaluate the distortion of the reconstructed video frames at the pixel level. In recent years, an increasing body of research has demonstrated that these quantitative metrics are insufficient in capturing the reconstruction quality perceived by humans [26]. In our study, besides PSNR, we employ the learned perceptual image patch similarity (LPIPS) metric [27] as well as the Fre ́chet video distance (FVD) metric [28] that have been popular for evaluating the perceptual qualities of generated video sequences. For all the videos, we computed the average metrics at the various bpp levels and provided a rate-distortion (perception) assessment of the compression performance. We report the average values and variances of the evaluation metrics.
Implementation Details: We employed pre-trained MCVD models from the training set and evaluated the model’s performance on the test set. For intra-frame compression, we employed the ELIC [22]. We set the threshold for frame generation based on the LPIPS metric, with threshold values within the empirical range of [0.02, 0.30]. After processing each video sequence with the proposed compression scheme, we selected the optimal combination of ELIC compression quality and LPIPS threshold to represent the optimal performance of the proposed method. Baseline: In addition to traditional video compression standards H.264 and H.265, we also compared the video compression performance with a SOTA neural compression
Methods PSNR (↑) LPIPS (↓) FVD (↓)
H.265 22.44 ± 2.18 0.22 ± 0.04 3886.70 ± 1174.39 H.264 24.74 ± 2.48 0.13 ± 0.04 2414.48 ± 830.35 Ours 24.37 ± 2.57 0.10 ± 0.03 737.96 ± 274.48 DCVC-DC 34.68 ± 2.01 0.04 ± 0.02 745.86 ± 401.16
Table 1: Comparison of PSNR, LPIPS, and FVD for groundtruth videos and different approaches. We report metrics at bpp = 0.06 here. Note that H.265 cannot reach bpp as low as 0.05. Similarly, DCVC-DC also fails to compress to a bitrate of 0.05 in certain videos.
method DCVC-DC[12]. For each video sequence in the Cityscapes dataset, we transform the processed video frames into YUV420p format, then conduct compression and decompression operations. In the case of grayscale datasets, due to the inherent support limitations of the libx264 codec, we convert the grayscale video frames into YUV420 format as YUV raw video for compression and decompression. For the evaluation, we only compute the metrics on the Y-component, therefore this approach could lead to higher compression video bit rates. For the H.264 and H.265 encoders, we control the generated video quality through the CRF parameter, which ranges from 0 to 51.
4.2. Results
We visualize the results of our compression scheme in Figs. 3 and 4 of the Cityscapes and SMMNIST datasets, respectively. The first and second rows show the original frames and reconstructions, respectively, using H.264 as a baseline. The bottom row shows the reconstructed video frames using our method. It can be observed in Fig. 3 that, our approach faithfully recovers the objects and textures while the standard codec suffers from blurriness, and in Fig. 4, the last frames using our method still capture the shapes and positions of the original digits accurately, despite stochastic movements. The rate-distortion-perception performances on the Cityscapes dataset are presented in Fig 2. We report both the distortion metric PSNR (Fig. 2a) and the perceptual metrics LPIPS (Fig. 2b) and FVD (Fig. 2c) that are more aligned with human perception under different compression rates in the low bpp regime. The orange and green curves correspond to the H.264 and H.265 video compression standards, the red curve represents the experimental results of DCVC-DC, and the blue curve corresponds to the results of our method. In our experiments,we observed that while DCVC-DC performs well on high-resolution datasets, its compression capability is somewhat limited on low-resolution images, with a maximum achievable bitrate around 0.06. Both H.264 and H.265 can only achieve a bpp as low as 0.04 and 0.06 respectively, whereas our method achieves lower bpp (0.02) and consistently performs better in terms of the FVD metric. For the


Data
Metrics PSNR LPIPS FVD
Bpp=0.06 Ours H.264 H.265 DCVC-DC Ours H.264 H.265 DCVC-DC Ours H.264 H.265 DCVC-DC UVG-YachtRide 25.61 26.44 23.95 35.20 0.096 0.075 0.14 0.009 2540 1958 4282 218 UVG-Beauty 28.45 28.76 25.28 nan 0.057 0.086 0.17 nan 1416 1913 3227 nan UVG-Bosphorus 24.55 29.57 26.70 nan 0.101 0.053 0.104 nan 2079 2275 2951 nan UVG-HoneyBee nan 25.37 21.55 nan nan 0.047 0.196 nan nan 554 1446 nan UVG-Jockey 22.82 22.91 20.95 nan 0.147 0.124 0.201 nan 2349 4194 6426 nan UVG-ReadySteadyGo 20.70 22.84 20.11 33.01 0.112 0.155 0.316 0.032 2832 3382 6347 902 UVG-ShakeNDry 24.68 26.59 24.43 36.68 0.111 0.077 0.158 0.0257 1400 2126 2896 689 AverageValue 24.47 30.41 23.28 34.96 0.104 0.088 0.184 0.022 2087 2343 4010 603
Table 2: On the UVG dataset, a comparison of PSNR, LPIPS, and FVD among actual videos and different methods is conducted. NaN values indicate that the method cannot achieve the specified bpp value on this particular video.
(a) PSNR (↑) (b) LPIPS (↓) (c) FVD (↓)
Fig. 2: Rate-distortion (perception) performance on the Cityscapes dataset.
other perceptual measure LPIPS, the proposed method still outperforms H.265 and obtains similar performance to H.264. For the distortion metric PSNR, our method still shows comparable results concerning the standard codecs. In Table 1, we report the distortion and perceptual qualities of the reconstructed video frames at the limiting bpp level (0.06) for the standard codecs. At the same bit rate, our method achieves better perceptual quality compared other methods. In terms of the model’s generalization ability, we also evaluated the model’s performance on the UVG dataset in Table 2. Since the Cityscapes dataset has relatively monotonous colors and small motion variations, when faced with highly diverse samples, the generative capability, and hence, the compression performance, may degrade.
4.3. Conclusion and Future Direction
We proposed a novel predictive video compression scheme that achieves ultra-low bit rates with visually pleasing reconstruction results by combining a neural image compression model with a SOTA generative model. In the proposed method, we sequentially decide a set of input frames to be encoded using neural codecs depending on whether the remaining frames can be generated by the decoder using
previously encoded frames. Each time, only a portion of the frames is encoded, and these frames are further used as conditional frames for the video generation model. Experimental results demonstrate that our model outperforms traditional video compression standards such as H.264 and H.265 in terms of perceptual qualities on datasets including SMMNIST and Cityscapes at ultra-low bpp. Our framework can serve as the basis for future works on predictive video compression employing different generative models at the receiver, and significantly reduce the required bitrate for video compression and transmission.
We remark that, in the current implementation, the generation process has to be carried out both at the encoder and the decoder. The encoder employs the generation process to determine the quality of the generated video frames, which is then used to decide which video frames to be compressed as intra frames. This increases the encoding complexity of the proposed scheme; however, simpler prediction models will be explored as part of our future work to determine which frames should be compressed.


t=1 t=2 t=3 t=4 t=5 t=6 ... t=28 t=29 t=30
Ground
Truth
H.264
Ours
Fig. 3: Visual comparison with state-of-the-art codec on Cityscape dataset. We present the first 6 frames and last 3 frames between original video frames (top row), H.264 codec (second row) and the proposed compression scheme (bottom row). Both H.264 compression and the videos compressed by our model are controlled to have a bpp of 0.07.Our model utilizes the first two frames to autoregressively generate some following frames based on an LPIPS threshold of 0.16.
t=1 t=2 t=3 t=4 t=5 t=6 ... t=28 t=29 t=30
Ground
Truth
Ours H.264
Fig. 4: Visual comparison with state-of-the-art codec on SMMNIST dataset. We present the first 6 frames and last 3 frames between original video frames (top row), H.264 codec (second row) and the proposed compression scheme (bottom row). Both H.264 compression and the videos compressed by our model are controlled to have a bpp of 0.04,LPIPS threshold is 0.16. We generate frames conditioning on 5 frames.
References
[1] Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander Anderson, and Lubomir Bourdev, “Learned video compression,” in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 3453–3462.
[2] Vijay Veerabadran, Reza Pourreza, Amirhossein Habibian, and Taco Cohen, “Adversarial distortion for learned video compression,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2020, pp. 640–644.
[3] Dengchao Jin, Jianjun Lei, Bo Peng, Zhaoqing Pan, Li Li, and Nam Ling, “Learned video compression with efficient temporal context learning,” IEEE Transactions on Image Processing, vol. 32, pp. 3188–3198, 2023.
[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis, “Align your latents: High-resolution video synthesis with latent diffusion models,” in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 22563–22575.
[5] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet, “Video diffusion models,” arXiv:2204.03458, 2022.
[6] Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer, and Karen Simonyan, “Transformation-based adversarial video prediction on large-scale data,” CoRR, vol. abs/2003.04035, 2020.
[7] Qiangeng Xu, Hanwang Zhang, Weiyue Wang, Peter N. Belhumeur, and Ulrich Neumann, “Stochastic dynamics


for video infilling,” in 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), 2020, pp. 2703–2712.
[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel, “Denoising diffusion probabilistic models,” in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, Eds. 2020, vol. 33, pp. 6840–6851, Curran Associates, Inc.
[9] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole, “Score-based generative modeling through stochastic differential equations,” in International Conference on Learning Representations, 2021.
[10] Jiahao Li, Bin Li, and Yan Lu, “Deep contextual video compression,” Advances in Neural Information Processing Systems, vol. 34, pp. 18114–18125, 2021.
[11] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao, “Dvc: An end-to-end deep video compression framework,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 11006–11015.
[12] Jiahao Li, Bin Li, and Yan Lu, “Neural video compression with diverse contexts,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22616–22626.
[13] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz, “Mocogan: Decomposing motion and content for video generation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 1526–1535.
[14] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song, “Denoising diffusion restoration models,” Advances in Neural Information Processing Systems, vol. 35, pp. 23593–23606, 2022.
[15] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt, “Diffusion probabilistic modeling for video generation,” arXiv preprint arXiv:2203.09481, 2022.
[16] Tobias Ho ̈ppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi, “Diffusion models for video prediction and infilling,” Transactions on Machine Learning Research, 2022.
[17] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal, “Mcvd-masked conditional video diffusion for prediction, generation, and interpolation,” Advances in Neural Information Processing Systems, vol. 35, pp. 23371–23385, 2022.
[18] Fabian Mentzer, Eirikur Agustsson, Johannes Balle ́, David Minnen, Nick Johnston, and George Toderici, “Neural video compression using gans for detail synthesis and propagation,” in Computer Vision – ECCV 2022, Shai Avidan, Gabriel Brostow, Moustapha Cisse ́, Giovanni Maria Farinella, and Tal Hassner, Eds., Cham, 2022, pp. 562–578, Springer Nature Switzerland.
[19] Ruihan Yang and Stephan Mandt, “Lossy image compression with conditional diffusion models,” arXiv:2209.06950, 2023.
[20] Noor Fathima Ghouse, Jens Petersen, Auke Wiggers, Tianlin Xu, and Guillaume Sautie`re, “A residual diffusion model for high perceptual quality codec augmentation,” arXiv:2301.05489, 2023.
[21] E. Hoogeboom, E. Agustsson, F. Mentzer, L. Versari, G. Toderici, and L. Theis, “High-fidelity image compression with score-based generative models,” arXiv:2305.18231, 2023.
[22] Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang, “Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding,” in IEEE/CVF Conf. on Computer Vision and Pattern Recog., 2022, pp. 5718–5727.
[23] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov, “Unsupervised learning of video representations using lstms,” in International conference on machine learning. PMLR, 2015, pp. 843–852.
[24] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele, “The cityscapes dataset for semantic urban scene understanding,” in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[25] Alexandre Mercat, Marko Viitanen, and Jarno Vanne, “Uvg dataset: 50/120fps 4k sequences for video codec analysis and development,” in 11th ACM Multimedia Systems Conf., 2020, pp. 297–302.
[26] Yochai Blau and Tomer Michaeli, “Rethinking lossy compression: The rate-distortion-perception tradeoff,” in International Conference on Machine Learning. PMLR, 2019, pp. 675–685.
[27] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
[28] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphae ̈l Marinier, Marcin Michalski, and Sylvain Gelly, “Fvd: A new metric for video generation,” 2019.