EXTREME GENERATIVE IMAGE COMPRESSION BY LEARNING
TEXT EMBEDDING FROM DIFFUSION MODELS
A PREPRINT
Zhihong Pan, Xin Zhou, Hao Tian Baidu Research (USA)
ABSTRACT
Transferring large amount of high resolution images over limited bandwidth is an important but very challenging task. Compressing images using extremely low bitrates (<0.1 bpp) has been studied but it often results in low quality images of heavy artifacts due to the strong constraint in the number of bits available for the compressed data. It is often said that a picture is worth a thousand words but on the other hand, language is very powerful in capturing the essence of an image using short descriptions. With the recent success of diffusion models for text-to-image generation, we propose a generative image compression method that demonstrates the potential of saving an image as a short text embedding which in turn can be used to generate high-fidelity images which is equivalent to the original one perceptually. For a given image, its corresponding text embedding is learned using the same optimization process as the text-to-image diffusion model itself, using a learnable text embedding as input after bypassing the original transformer. The optimization is applied together with a learning compression model to achieve extreme compression of low bitrates <0.1 bpp. Based on our experiments measured by a comprehensive set of image quality metrics, our method outperforms the other state-of-the-art deep learning methods in terms of both perceptual quality and diversity.
1 Introduction
With the increasing amount of image streams available for broad range of applications, lossy image compression is a very useful technique for efficient image storage and transmission. Over the years, various engineered codes such as JPEG [30], JPEG2000 [52], and the more recent BPG[4] have been proposed to compress single images but their performance have saturated overall. More recently, deep learning based image compression methods have been studied [3, 36, 7]. These models are generally trained in an end-to-end fashion to minimize a rate-distortion object R + λD. Here R represents the entropy of latent representations which is estimated by an entropy model, D is the difference between the original image and the compressed one, and λ determines the desired trade-off between rate and distortion. When λ is small, the optimization gives higher priority to compression rate so the resulted bitrate (evaluated as bits-per-pixel, bpp) is low. Consequently, the compressed image has lower quality due to higher D loss term. With accuracy metrics like mean squared error (MSE) and multi-scale structural similarity (MS-SSIM) are often used for D, the low quality compressed images are usually blurry. For extremely low bitrates (<0.1 bpp), both engineered codecs and deep learning compression models are subject to very poor perceptual qualities.
To tackle this problem, some recent methods [61, 63, 29, 35] aim to restore less blurry image from highly compressed latent representations at the cost of accuracy. These model adopt generative adversarial networks (GAN) [19] to fully or partially replace the accuracy metrics in D with discrimination loss so they can generate sharp and realistic images even at very low bitrates. For the challenging task of extremely low bitrates, GAN is further exploited in more recent studies [2, 11, 25] to restore sharp images with minimized distortion and visual artifacts. However, they all inherit the drawback of unstable training from GAN, making it difficult to tune the training process for large datasets. In this paper, we propose the first generative image compression method with extremely low bitrates using denoising diffusion models. As it utilizes an existing text-to-image model which is already trained with a gigantic dataset, it is applicable to any type of image with no need of further tuning.
Similar to GAN a few years back, denoising diffusion models [53, 22, 54] are gaining popularity increasingly for their advantages in generating images with high qualities in both fidelity and diversity without disadvantage of unstable training like GAN. In addition to unconditional image generation, diffusion models have also empowered the breakthrough developments in diffusion-based text-to-image generation models [47, 38, 43, 49] which are able to create
arXiv:2211.07793v1 [eess.IV] 14 Nov 2022


A PREPRINT - NOVEMBER 14, 2022
Original Iwai et al. [25] Mentzer et al. [35] Ours
Figure 1: Visual examples of generated images after compression with extremely low bitrates, demonstrating our method’s superior capability to generate very sharp details.
realistic images according to given text descriptions. They often use existing transformer models to encode text prompts as textual embeddings and use them as conditions in training and sampling of the diffusion model. Realising the power of these model in turning short texts into high resolution and high quality images, the generative image compression we propose here encodes an input image as a textual embedding which is quantized and compressed for storage. At inference time for decompression, the compressed textual embedding is decoded and used as the conditional input for image generation. While sampling from the diffusion model can generate high quality images, the random sampling could lead to diverse outcomes without guarantee of resembling the original input. To ensure the generation success, the original image is also compressed from an existing learning based compression model and it is used as a guidance at sampling time, on top of the classifier-free guidance. The additional bitrates for compression guidance is only 0.01 − 0.02 bpp so the overall bitrates is still very low. Note that there are a couple of newly proposed methods [58, 65] which are concurrently working on lossy image compression using diffusion models. However, not like our method here, they both need ground-up training of a dedicated diffusion model and do not address the challenge of extremely low bitrates. Based on our experiments, our proposed model is capable of generating image of the highest perceptual quality while maintaining overall resemblance with the original image. As shown in Fig. 1, the other state-of-the-art methods [25, 35] are subject to blurry artifacts when the bitrate gets below 0.1 while our method is able to generate very sharp details. In the example of snowy mountains, our generated sample has details sharper even than the original. Although details like the number of snowy tracks are different, which results in poor measurements in terms of pixel by pixel accuracy, our sample is highly photo-realistic and look like the same image as the original overall.
In summary, we propose an innovative generative image compression method with the following main contributions:
• Using existing text-to-image diffusion models, our method can compress an input image as a textual embedding of extremely low bitrates and later generate diverse diverse sharp images which resemble the input perceptually.
• A hybrid guidance method is studied to combine classifier-free guidance from pre-trained text-to-image models and newly introduced compression guidance for optimal generation results.
• The number of bits needed to compress the textual embedding is largely independent of image content and resolution, so the bitrate is relatively constant for a fixed resolution and decrease when the image resolution increases.
2 Related Works
2.1 Image Compression
Shannon’s theory of communication [51] has provided the fundamental basis for the coding theory used in classical image compression methods. Using explicit probabilistic modeling and feature extractions, various codes, like JPEG [30], BPG [4] and WebP [20], have been effectively engineered for the task of image compression. The earliest learning based image compression methods [59, 60] relied on RNNs [34]. Ballé et al. [3] were the first to introduce an end-to-end autoencoder and entropy model that jointly optimizes rate and distortion, which was then enhanced with a scale hyperprior in [3] to capture spatial dependencies in the latent representation. Later various autoregressive and hierarchical priors [36, 24] were introduced to further improve the compression performance. Cheng et al. [7] added attention modules and used a Gaussian Mixture Model (GMM) to estimate the latent representation distribution for further improvements.
2


A PREPRINT - NOVEMBER 14, 2022
zT
Diffusion Decoder
zo
xo #xg
$ex
Textual Embedding
Compressor
z0
t x0
t #x0
t Guidance
≈0.06bpp
≈0.01bpp
Figure 2: Overview of the sampling process of proposed generative image compression using two inputs of extremely low bitrates, where eˆx, a highly compressed textual embedding, is used as the conditional input for a pre-trained latent diffusion model, and xˆg, a highly compressed image from original image x, is used a constraint to guide the
intermediate latent image zt0 at each time step t. These two are saved after the initial compression process and are the only two needed to reconstruct a high quality image x0.
Since their introduction in [19], GANs have progressed greatly in unconditional and conditional image generation of high resolution photo-realistic images [14, 62, 27, 26]. The adversarial loss function was first introduced in an end-to-end framework [46] for improved perceptual quality, and has been continuously improved in following studies [50, 61]. While these methods are capable of reconstructing photo-realistic image with very low bitrate, generative image compression with extremely low bitrates (<0.1 bpp) was first studied in [2] and further improved in following studies [11, 25]. Comparing to these GAN based extreme generative compression models, ours is the first to utilize diffusion models to tackle this challenging task.
2.2 Denoising Diffusion Models
Inspired by non-equilibrium thermodynamics [53], the denoise diffusion models define a Markov chain of diffusion steps to slowly add random noise to data so the intractable real data distribution is transformed to a tractable one like Gaussian. Then the models learn to reverse the diffusion process to construct desired data samples from randomly sample Gaussian noise. Ho et al. [22] proposed a denoising diffusion probabilistic model (DDPM) to interpret the reverse diffusion process as a large amount of consecutive denoising steps following conditional Gaussian distribution. Alternatively, Song et al. [55, 56] used stochastic differential equations to model the reverse diffusion process and developed a score-based generative model to produce samples via Langevin dynamics using estimated gradients of the data distribution. Later numerous methods [39, 54, 33] have been proposed to use much fewer denoising steps without significant degradation in image quality. To improve image quality, Dhariwal et al. [12] proposed a classifier guidance method to iteratively modify the denoised step using a gradient calculated from a retrained noisy classifier. Later Ho et al. [23] invented a classifier-free guidance method that trains a conditional model using randomly masked class labels and treat the difference between conditional and unconditional sampling at inference time as a proxy classifier. The compression guidance proposed here is applied similarly as the classifier guidance.
In recent years, GAN based deep learning models have been successful used for various generative tasks [14, 62, 27], including text-to-image generations [45, 66, 64, 41, 57, 17]. More recently, autoregressive (AR) models have also shown promising results in image generation [40, 6, 15]. For text-to-image generations, various frameworks, including DALL-E [44], CogView [13] and M6 [31], have been proposed to use large transformer structure to model the joint distribution of text and image tokens. Diffusion models have progressed rapidly to set state-of-the-art for many generative tasks, including text-to-image generations. Previously, text-to-image generation are dominated by GANs [45, 66, 64, 41, 57, 17] and autoregressive (AR) models [44, 13, 31]. Most recently, diffusion-based text-to-image generation has been a red hot research topic in both the academia and industry. Initially, an unconditional diffusion model [9] was demonstrated highly capable of text-to-image generation using sampling guidance to match the CLIP scores[42] of the text input and generated image. More recent models all use transformer based text embedding to train the conditional diffusion model, generating either a low-resolution image [38, 49] or an image embedding [49] before generating the full resolution output. Alternatively, Rombach et al. [47] proposed to conduct the conditional text-to-image diffusion in a latent space of reduced resolution for faster training and sampling. Based on that, a large text-to-image model, Stable Diffusion [10], is trained with a huge dataset and released for open research. Our proposed image compression method is validated using the released version v1-4.
3


A PREPRINT - NOVEMBER 14, 2022
Sc↓ 100 18.8 19.0 18.8 18.8 19.0 18.8 0.71 0.72 0.72 0.72 0.72 0.72 0.38 0.38 0.37 0.37 0.36 0.38 3.87 3.88 3.80 3.84 3.60 3.92 140 19.2 19.5 19.5 19.6 19.3 19.3 0.72 0.73 0.74 0.73 0.72 0.72 0.37 0.37 0.35 0.35 0.38 0.39 3.91 3.91 3.83 3.77 3.96 4.29 180 19.8 19.8 19.7 19.9 19.9 19.8 0.73 0.73 0.74 0.74 0.73 0.73 0.36 0.36 0.37 0.35 0.39 0.41 3.94 3.64 4.11 3.90 4.36 4.42 220 20.0 20.1 20.0 20.2 20.1 19.9 0.73 0.74 0.74 0.74 0.74 0.73 0.37 0.37 0.35 0.38 0.41 0.45 4.02 3.84 3.68 4.26 4.47 4.90 260 20.3 20.5 20.4 20.3 20.5 20.3 0.74 0.75 0.75 0.74 0.74 0.73 0.40 0.40 0.37 0.40 0.43 0.48 4.14 4.35 4.00 4.42 4.69 5.44 300 20.5 20.7 20.6 20.6 20.5 20.4 0.74 0.75 0.75 0.75 0.74 0.73 0.40 0.40 0.39 0.40 0.46 0.52 4.34 4.40 4.36 4.62 5.45 6.00 350 20.6 20.7 20.6 20.8 20.6 20.6 0.74 0.75 0.75 0.74 0.73 0.73 0.44 0.44 0.45 0.46 0.52 0.53 5.02 4.83 5.02 5.37 6.03 6.22 400 20.7 20.8 20.8 20.9 20.9 20.9 0.73 0.74 0.74 0.75 0.74 0.73 0.50 0.50 0.48 0.49 0.52 0.54 5.66 5.89 5.70 5.86 6.26 6.62
Sf→ 0.70 0.85 1.00 1.20 1.40 1.60 0.70 0.85 1.00 1.20 1.40 1.60 0.70 0.85 1.00 1.20 1.40 1.60 0.70 0.85 1.00 1.20 1.40 1.60
PSNR FSIM LPIPS NIQE
Figure 3: Comparison of multiple image quality metrics for different combinations of compression guidance scale (sc) and classifier-free guidance scale (sf ). Red in the heatmaps means higher quality while green meas lower.
3 Proposed Method
Our proposed generative compression method is built on pre-trained text-to-image diffusion models without any change in their model weights. For demonstrative purposes, the publicly available Stable Diffusion model [10] which uses the latent diffusion model [47] architecture, is adopted for all experiments. In this case, diffusion process is conducted in the latent space z which can be decoded to the image space. For any input image x, it is first applied with a state-of-art learned compressor [7] to save as a highly compressed image xˆg which is used as a guidance during inference time explained later. It only consumes storage of a tiny portion of the original size, around 0.01 bpp. Additionally, adopting the textual inversion used in prior works [18, 48, 28], a textual embedding ex is found as the optimal conditional input to generate x from random noise zT using an iterative learning process. ex is then further quantized and compressed as eˆx using a compressor [7]. As the textual embedding has a fixed dimensionality in regardless of the image size, the bitrate needed to save eˆx depends on the original image size and it decreases when the number of pixels increases. For a 512 × 768 image like in the Kodak dataset [16], the bitrate of eˆx is around 0.06 bpp. For large images in need of extreme compression, the total bitrate is expected to be 0.07 or less. For the generation process of decompression, as shown in Fig. 2, the denoising step in latent space to transform random noise zT to a latent sample z0 is conditional to the compressed embedding eˆx. At the same time, for each denoising step t, the noise estimation is guided by correcting
the intermediate latent sample zt0 in reference to the compressed guidance xˆg. After generation of the latent sample z0, an output image x0 is reconstructed through the decoder in the latent diffusion model. Details of the textual inversion and compression guidance are included below.
3.1 Latent Diffusion Model Background
For any image x0, it is first encoded as a latent sample z0 and the distribution of z0 from all realistic images is denoted as q(z0). Each z0 can be transformed progressively as z1, ..., zT through added Gaussian noises and when T is large enough zT becomes a random noise. In order to convert any random noise zT to a latent sample z0, a diffusion model with parameter θ is designed to match the true posterior using:
pθ(zt−1|zt) = N (μθ(zt, t), Σθ(zt, t)). (1)
Here Σθ(zt, t) is often fixed as σtI in practice so the diffusion model is only trained for μθ(zt, t). Starting from a noise zT ∼ N (0, I), the learned posterior can be used to sample zt, t = z − 1, z − 2, ... iteratively. The output latent sample latent z0 can be decoded as a generated image x0.
As shown in DDPM [22], a re-weighted variational lower-bound (VLB) is used as an effective surrogate objective for diffusion model optimization. As μθ(zt, t) can be derived from θ(zt, t), estimated noise added in zt, a diffusion model can be be optimized using a simple standard mean-squared error (MSE) loss
Lsimple = Et,z0, || − θ(zt, t)||2 (2)
where is the known Gaussian noise added to z0 to synthesize zt.
3.2 Compression Guidance
In order to improve image sampling quality at inference time, Dhariwal et al. [12] proposed a classifier guidance method to perturb the estimated mean by adding the gradient of the log-probability log pφ(y|zt) of a target class y predicted by a classifier where zt needs to be decoded as an image xt, denoted as xt = d(zt) where d stands for the decoding process, before feeding to classifier in the case of of latent diffusion. The resulting new perturbed mean μˆθ(zt, t|y) is given by
μˆθ(zt, t|y) = μθ(zt, t|y) + sΣθ(zt, t|y)∇zt log pφ(y|zt) (3)
4


A PREPRINT - NOVEMBER 14, 2022
where coefficient s is called the guidance scale. A larger s leads to higher sample quality but less diversity. φ represents the classifier parameters which can be further refined with noisy images and conditional to t as xt is normally noisy.
For our proposed compression guidance, the reference to use in place of label y is an extremely low bitrate image xˆg, which is compress from x as xˆg = c(x). Similar to classifier guidance, the estimated mean during the reverse denoising process is perturbed by the gradient of the difference between xˆg and compressed xt:
μˆθ(zt, t) = μθ(zt, t) − sΣθ(zt, t)∇zt |xˆg − xˆt| (4)
where xˆt = d(c(zt)). However, unlike classifier guidance where φ can be optimized for noisy images, there is no learnable variable to optimize in the case of our compression guidance. To mitigate the impact of noise present in xˆt,
here we propose an alternative guidance method to calculate the perturbing gradient by comparing the "noise-free" xˆt0 and reference xˆg instead
xt
0 = c(d((zt − √1 − α ̄t θ(zt, t))/√α ̄t))
μˆθ(zt, t) = μθ(zt, t) − scΣθ(zt, t)∇zt |xˆg − xˆt| (5)
where αt is set from pre-determined noise schedule, and sc is the compression guidance scale, to differentiate from the classifier-free guidance [23] which is adopted in the diffusion model used for our experiments. The scale used in classifier-free guidance is denoted as sf . As both guidance methods are used together, we empirically studied the optimal settings for both sc and sf for best effects of this hybrid guidance.
3.3 Textual Inversion
For the adopted textual inversion, the goal is to find an optimal textual embedding ex backwards from a given image x. This process is a learning process to to minimize the following expected error:
Et, || − θ(zt, t, ex)||2 (6)
where θ is the diffusion model pre-trained with the loss term defined in Equation 2. with fixed weights θ and x is fixed too. By optimizing iteratively using varying and t, the target textual embedding ex can be learned effectively. For any image x, the embedding ex has a fixed number of T tokens and each token is embedded as a N -dimensional vector. To effectively compress the T × N real numbers to meet needs of our extreme compression application, ex is quantized and compressed with an existing compression model [7], denoted as eˆx. To further optimize the whole process, the quantization and compression process are included in the learning process of textual inversion by minimizing the following error instead
Et, || − θ(zt, t, eˆx)||2. (7)
4 Experiments
All experiments in this study are conducted using pre-trained Stable Diffusion [10] model. For xˆg used in compression guidance, the original image x is first downsampled with a scale of ×4 before compressed using an existing compression model [7] with GMM and attention. For textual inversion, we use 64 embedded vectors, each has 768 elements. For compression of said textual embedding using the same existing compression model [7], it is reshaped as a RGB color image of 64 × 256 pixels.
To assess the effectiveness of our proposed method, we use the photo-realistic images from the Kodak dataset [16] to conduct the compression experiments. For each image, the optimal compressed textual embedding is determined using 4000 iterative learning steps before tested for image generation. To generate high quality image at inference time, we use 100 DDIM [54] sampling steps (η = 1) for the diffusion model. To assess the image quality quantitatively, a comprehensive set of metrics are used. The first set of metrics use the original image as the ground-truth (GT) reference. In addition to the standard PSNR metric, FSIM [67] is chosen as a measure relying on low-level features the human visual systems often use. and LPIPS [68] is also used for its effectiveness as a perceptual metric. For blind metrics without GT reference, NIQE [37] is the one comparing image statistics with those of undistorted images. FID [21] and KID [5] are also no-reference metrics, calculated from statistics of learned features. The are both chosen for their popular application on generative models and KID is known to be more robust as an unbiased one.
4.1 Hybrid Guidance
For the classifier-free guidance included in the diffusion model, it is generally known that higher guidance scale improves generated image quality. However, with the introduction of compression guidance in our method, it is useful to validate the effects of different compression guidance scales by themselves, as well as in combination of
5


A PREPRINT - NOVEMBER 14, 2022
Table 1: Quantitative image quality comparison of generative compression methods using Kodak dataset (best of three marked in red).
A bpp PSNR↑ FSIM↑ LPIPS↓ NIQE↓ FID↓ KID↓
OriginalA - - - - 3.020 258.5 3.157 Iwai et al. [25]A 0.063 ± 0.028 24.39 0.8794 0.3054 3.566 269.1 5.379 Mentzer et al. [35]A 0.075 ± 0.021 24.93 0.8906 0.2037 3.563 264.2 5.471 OursA 0.070 ± 0.008 20.61 0.7486 0.3611 3.731 258.6 3.960
Original Iwai et al. [25] Mentzer et al. [35] Ours
Figure 4: Visual examples of generated images after extreme compression. Our model has the average bitrate for better performance while two competitive models are subject to severe artifacts due to abnormally low bitrates, a common disadvantage of prior works where models are only trained for a target average bitrate over a large training set.
different classifier-free guidance scales. As shown in Fig. 3, we have selected four different image metrics to cover both image reconstruction accuracy and perceptual quality, where PSNR is used to measure reconstruction accuracy, FSIM and LPIPS are chosen for both accuracy and perceptual quality and NIQE is for perceptual quality only. All experiments are conducted on the Kodak dataset. In general, higher compression guidance scale sc is preferred for reconstruction accuracy but less favorable for perceptual quality. While for classifier-free guidance scale sf , it does not have a significant impact on accuracy for the range we tested on while it shows an optimal value slightly less than 1 for all perceptual quality related metrics. As a larger value like 5 is often recommended for the classifier-free guidance scale sf when used for its original generation applications, this is the first observation that a smaller values less than 1, probably caused by introduction of compression guidance. For final experiments, sc and sf are set empirically as 215 and 0.95 for the best trade-off between reconstruction accuracy and perceptual quality.
4.2 Image Quality Assessment
We have validated our method in comparison with other state-of-art methods using a comprehensive set of image quality metrics. For HiFiC developed by Mentzer et al. [35], the one with lowest bitrate available is higher than 0.1 bpp. For fair comparison, we have retrained the model with a target bitrate of 0.07 bpp using a large set of high quality images, including both DIV2K and Flickr2K as used in [1]. The other one proposed by Iwai et al. [25] is trained with a diverse and large dataset COCO [32] so the pre-trained model is used for direct comparison. For two other methods proposed by Agustsson et al. [2] and Dash et al. [11], the models available are trained the Cityscapes dataset [8], which is limited to city scenes and not sharp enough comparing to photo-realistic images included in Kodak dataset. As they are earlier models than the two included for comparison here, they are not retrained to be included here for assessment.
As shown in Table 1, our method is the best in perceptual image quality metrics when no ground-truth reference is available, including NIQE, FID and KID. For these three metrics without references, the original uncompressed Kodak
6


A PREPRINT - NOVEMBER 14, 2022
Original Iwai et al. [25] Mentzer et al. [35]
Ours #1 Ours #2 Ours #1
Figure 5: Visual examples of generated images. Multiple samples from our model using the same compressed source enjoy both high perceptual quality and diversity.
images are also assessed to compare with the compressed results. It shows that the original image indeed have higher perceptual quality compared to compressed ones. For accuracy related metrics, especially PSNR which is the least related to perceptual quality, our method is not as impressive as the peers. In addition to superior perceptual quality, our method has another advantage of near-constant bitrate. As included in Table 1, it has a much smaller standard deviation of 0.008 while the other two are 0.021 and 0.028. This is useful in applications where transmission bandwidth is very limited and an accurate estimation of bits needed for storage of image(s) is important before compression. While the average bitrate for the full Kodak dataset is similar for all three models, the bitrates for the image shown in Fig. 4 are significantly below average for the two model other than ours. As a result, generated images from both models are subject to severe blurry artifacts. In the case of [25] which has a 0.04 bpp bitrate, it has additional false color artifacts where the full face turns reddish. In contrast, our method has a sufficient 0.07 bpp bitrate and is able to generate sharp details.
4.3 Generation Diversity
While the two competitive models are generative models based on GAN, they are not able to generate high quality images with large and realistic variations. In comparison, our model is able to generate photo-realistic images with large diversity in details while maintaining overall consistency. As shown by the three sample from our model in Fig. 5, there are large variations in both foreground and background areas. For the foreground example, the red line patterns
7


A PREPRINT - NOVEMBER 14, 2022
are consistent overall but vary greatly in details like line sizes and locations, and all three have highly focused sharpness. In the other example, our samples all have smooth out-of-focus background yet are quite different from each other.
5 Conclusions
In this paper, we present a generative image compression model capable of encoding high resolution image with extremely low bitrates of 0.07 bpp. It is the first such model built on top of pre-trained text-to-image diffusion models. Comparing to similar works using GAN, it has some distinctive advantages: first, it is able to generates diverse images from one compressed source, all with higher perceptual quality and overall resemblance with the source image; secondly, it does not need training on a dedicated dataset; lastly, it has a relatively fixed bitrate for different images, while others models suffer from a large variation in bitrates.
In terms of computational efficiency, both compression and decompression steps of our proposed method are more time consuming in general. For compression, an iterative learning process is needed to find the optimal compressed textual embedding, while for decompression a large number of denoising steps are needed for high quality outputs. Beside pure research interest, findings in this study are relevant for some real world applications where it is not limited by computational resources at both server and client sides but extremely limited in communication bandwidth.
References
[1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 126–135, 2017.
[2] Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Generative adversarial networks for extreme learned image compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 221–231, 2019.
[3] Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. arXiv preprint arXiv:1802.01436, 2018.
[4] Fabrice Bellard. BPG image format. https://bellard.org/bpg/.
[5] Mikołaj Bi ́nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018.
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 1691–1703, 2020.
[7] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7939–7948, 2020.
[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213–3223, 2016.
[9] Katherine Crowson, Maxwell Ingham, Adam Letts, and Alex Spirin. Disco Diffusion. https://github.com/ alembics/disco-diffusion, 2022.
[10] Katherine Crowson, Maxwell Ingham, Adam Letts, and Alex Spirin. Stable diffusion. https://github.com/ CompVis/stable-diffusion, 2022.
[11] Shubham Dash, Giridharan Kumaravelu, Vijayakrishna Naganoor, Suraj Kiran Raman, Aditya Ramesh, and Honglak Lee. CompressNet: Generative compression at extremely low bitrates. In 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 2314–2322. IEEE, 2020.
[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Advances in Neural Information Processing Systems, 34:8780–8794, 2021.
[13] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. CogView: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822–19835, 2021.
[14] Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. arXiv preprint arXiv:1802.04208, 2018.
8


A PREPRINT - NOVEMBER 14, 2022
[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873–12883, 2021.
[16] Richard W. Franzen. Kodak lossless true color image suite. http://r0k.us/graphics/kodak/.
[17] Stanislav Frolov, Tobias Hinz, Federico Raue, Jörn Hees, and Andreas Dengel. Adversarial text-to-image synthesis: A review. Neural Networks, 144:187–209, 2021.
[18] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.
[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014.
[20] Google. An image format for the web;. https://developers.google.com/speed/webp/.
[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020.
[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.
[24] Yueyu Hu, Wenhan Yang, and Jiaying Liu. Coarse-to-fine hyper-prior modeling for learned image compression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11013–11020, 2020.
[25] Shoma Iwai, Tomo Miyazaki, Yoshihiro Sugaya, and Shinichiro Omachi. Fidelity-controllable extreme image compression with generative adversarial networks. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 8235–8242. IEEE, 2021.
[26] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. Advances in Neural Information Processing Systems, 34:852–863, 2021.
[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401–4410, 2019.
[28] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276, 2022.
[29] Jooyoung Lee, Donghyun Kim, Younhee Kim, Hyoungjin Kwon, Jongho Kim, and Taejin Lee. A training method for image compression networks to improve perceptual quality of reconstructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 144–145, 2020.
[30] Alain M Leger, Takao Omachi, and Gregory K Wallace. Jpeg still picture compression algorithm. Optical Engineering, 30(7):947–954, 1991.
[31] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, et al. M6: A Chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823, 2021.
[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014.
[33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.
[34] Larry R Medsker and LC Jain. Recurrent neural networks. Design and Applications, 5:64–67, 2001.
[35] Fabian Mentzer, George D Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compression. Advances in Neural Information Processing Systems, 33:11913–11924, 2020.
[36] David Minnen, Johannes Ballé, and George D Toderici. Joint autoregressive and hierarchical priors for learned image compression. Advances in neural information processing systems, 31, 2018.
[37] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a “completely blind” image quality analyzer. IEEE Signal processing letters, 20(3):209–212, 2012.
[38] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2022.
9


A PREPRINT - NOVEMBER 14, 2022
[39] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162–8171, 2021.
[40] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pages 4055–4064, 2018.
[41] Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao. Mirrorgan: Learning text-to-image generation by redescription. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1505–1514, 2019.
[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748–8763, 2021.
[43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.
[44] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831, 2021.
[45] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In International conference on machine learning, pages 1060–1069, 2016.
[46] Oren Rippel and Lubomir Bourdev. Real-time adaptive image compression. In International Conference on Machine Learning, pages 2922–2930. PMLR, 2017.
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022.
[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022.
[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.
[50] Shibani Santurkar, David Budden, and Nir Shavit. Generative compression. In 2018 Picture Coding Symposium (PCS), pages 258–262. IEEE, 2018.
[51] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948.
[52] Athanassios Skodras, Charilaos Christopoulos, and Touradj Ebrahimi. The jpeg 2000 still image compression standard. IEEE Signal processing magazine, 18(5):36–58, 2001.
[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265, 2015.
[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.
[55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.
[56] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.
[57] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. DF-GAN: Deep fusion generative adversarial networks for text-to-image synthesis. arXiv preprint arXiv:2008.05865, 2020.
[58] Lucas Theis, Tim Salimans, Matthew D Hoffman, and Fabian Mentzer. Lossy compression with gaussian diffusion. arXiv preprint arXiv:2206.08889, 2022.
[59] George Toderici, Sean M O’Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet Baluja, Michele Covell, and Rahul Sukthankar. Variable rate image compression with recurrent neural networks. arXiv preprint arXiv:1511.06085, 2015.
[60] George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and Michele Covell. Full resolution image compression with recurrent neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 5306–5314, 2017.
10


A PREPRINT - NOVEMBER 14, 2022
[61] Michael Tschannen, Eirikur Agustsson, and Mario Lucic. Deep generative models for distribution-preserving lossy compression. Advances in neural information processing systems, 31, 2018.
[62] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526–1535, 2018.
[63] Lirong Wu, Kejie Huang, and Haibin Shen. A GAN-based tunable image compression system. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2334–2342, 2020.
[64] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1316–1324, 2018.
[65] Ruihan Yang and Stephan Mandt. Lossy image compression with conditional diffusion models. arXiv preprint arXiv:2209.06950, 2022.
[66] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 5907–5915, 2017.
[67] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. FSIM: A feature similarity index for image quality assessment. IEEE transactions on Image Processing, 20(8):2378–2386, 2011.
[68] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image superresolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2472–2481, 2018.
11